{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "assignment5_read_mapping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lent3Ii326vK"
      },
      "source": [
        "# CS 425 fall 2021 Assignment 5\n",
        "\n",
        "\n",
        "## Read mapping\n",
        "\n",
        "In this assignment you will implement a read mapper based on suffix arrays and test it on a small dataset from the bacterium E. coli.\n",
        "\n",
        "The sequencing reads are in [FASTQ format](https://en.wikipedia.org/wiki/FASTQ_format).  Each FASTQ record is composed of four lines; for this work we will use the first line which contains read identifiers, and the third line which contains the sequence of the read.\n",
        "The reads should be aligned against the reference genome of the bacterium [E. coli](https://www.ncbi.nlm.nih.gov/nuccore/NC_000913).\n",
        "\n",
        "Note that each read can originate from the positive or negative strand.  So take that into account in the mapping process.\n",
        "\n",
        "For the output, create a tab delimited file, with a line for each read that you were able to match.  The columns for the file should be as follows:\n",
        "\n",
        "* Read identifier (from the FASTQ file)\n",
        "* The strand to which the read has been matched, represented by the symbols + or -.\n",
        "* The position of the first matching nucleotide in 0-based indexing.\n",
        "\n",
        "In you implementation use the read-mapping strategy we discussed in class that accounts for the possibility for errors by splitting a read into $k+1$ segments when allowing for up to $k$ errors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vQA7sIyIDYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e727ba5-da21-456c-9d4e-80f100f4c9e0"
      },
      "source": [
        "!pip install pandas\n",
        "import pandas as pd\n",
        "import csv\n",
        "from collections import defaultdict"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqFAtZUM26vN"
      },
      "source": [
        "Below is code for computing the suffix array that you can use in your code.  It has been tested to work for the *E. coli* genome in a very reasonable amount of time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOPX0hPs26vN"
      },
      "source": [
        "def read_fasta(file_name):\n",
        "    with open(file_name,\"r\") as f:\n",
        "    #read first line of description\n",
        "        description = f.readline().rstrip()\n",
        "\n",
        "    #read genome sequence\n",
        "        genome = \"\"\n",
        "        line = f.readline().rstrip()\n",
        "        while line != '':\n",
        "            genome += line\n",
        "            line = f.readline().rstrip()\n",
        "\n",
        "    return genome + '$'\n",
        "\n",
        "def read_FASTQ(file_name):\n",
        "  headers = []\n",
        "  sequences = []\n",
        "  seps = []\n",
        "  qualities = []\n",
        "\n",
        "  final = []\n",
        "\n",
        "  # add each section of the FASTQ file into its own list\n",
        "  with open(file_name, 'r') as ifh:\n",
        "    for header, sequence, sep, quality in zip(ifh, ifh, ifh, ifh):\n",
        "      headers.append(header.rstrip())\n",
        "      sequences.append(sequence.rstrip())\n",
        "      seps.append(sep.rstrip())\n",
        "      qualities.append(quality.rstrip())\n",
        "\n",
        "  # take all lists and combine them into a 2d list\n",
        "  for i in range(len(headers)):\n",
        "    # temp = [headers[i], sequences[i], seps[i], qualities[i]]\n",
        "    temp = [headers[i], sequences[i], seps[i]]\n",
        "    final.append(temp)\n",
        "\n",
        "  return final\n",
        "\n",
        "# part of creating suffix array\n",
        "def bucket_sort(s, bucket, order):\n",
        "    d = defaultdict(list)\n",
        "    for i in bucket:\n",
        "        key = s[i + order // 2:i + order]\n",
        "        d[key].append(i)\n",
        "    result = []\n",
        "    for k, v in sorted(d.items()):\n",
        "        if len(v) > 1:\n",
        "            result += bucket_sort(s, v, 2 * order)\n",
        "        else:\n",
        "            result.append(v[0])\n",
        "    return result\n",
        "\n",
        "# Searches through suffix Text to see if pattern is part of Text\n",
        "def binary_Search(sfxSorted, pattern, num_errors=0):\n",
        "  start = 0\n",
        "  end = len(sfxSorted) - 1\n",
        "  length = len(pattern)\n",
        "\n",
        "\n",
        "  while start <= end:\n",
        "    middle = (start + end) // 2\n",
        "    test = sfxSorted[middle]\n",
        "    midpoint = test[:length]\n",
        "\n",
        "    # Check number of errors\n",
        "    if (sum(c1!=c2 for c1,c2 in zip(pattern,midpoint)) == num_errors and num_errors > 0):\n",
        "      split_pattern = divide_string(pattern, num_errors+1)\n",
        "      split_text = divide_string(midpoint, num_errors+1)\n",
        "      \n",
        "      for i in range(len(split_text)):\n",
        "        if split_pattern[i] == split_text[i]:\n",
        "          return binary_Search(sfxSorted, split_pattern[i], 0)\n",
        "\n",
        "    # Else if midpoint is greater, ignore top/left half\n",
        "    elif(midpoint  > pattern):\n",
        "      end = middle - 1\n",
        "    \n",
        "    # Else if x is smaller, ignore bottom/right half\n",
        "    elif( midpoint < pattern):\n",
        "      start = middle + 1\n",
        "    \n",
        "    # Checks if pattern is present at mid\n",
        "    else:\n",
        "      return top_search(sfxSorted, pattern, middle)\n",
        "\n",
        "  return -1\n",
        "\n",
        "# Finds first occurence of pattern inside Suffix Text\n",
        "def top_search(sfxSorted, pattern, middle):\n",
        "  shorten = [x[:len(pattern)] for x in sfxSorted]\n",
        "  shorten = shorten[:middle+1]\n",
        "\n",
        "  end = len(shorten)-1\n",
        "  firstOccur = end\n",
        "\n",
        "  while pattern == shorten[end]:\n",
        "    firstOccur = end\n",
        "    end -= 1\n",
        "  \n",
        "  return firstOccur\n",
        "\n",
        "# If mismatch found then it breaks string up into n pieces\n",
        "def divide_string(string, n):\n",
        "  chunk = int(len(string) / n) + (len(string) % n > 0)\n",
        "  start = 0\n",
        "  store = []\n",
        "  for i in range(n):\n",
        "    if i == n-1:\n",
        "      store.append(string[start:])\n",
        "    else:\n",
        "      store.append(string[start:start+chunk])\n",
        "    start += chunk\n",
        "\n",
        "  return store\n",
        "\n",
        "# Reverse Complement\n",
        "def complement(genome):\n",
        "  bases = {'A':'T','T':'A','C':'G','G':'C'}\n",
        "  rev = []\n",
        "  for gene in genome:\n",
        "    rev += bases[gene]\n",
        "    \n",
        "  return ''.join(rev)[::-1]\n",
        "\n",
        "# creates Array of suffix index\n",
        "def suffix_array(s):\n",
        "  return bucket_sort(s, range(len(s)), 1)\n",
        "\n",
        "# Creates Array of suffix Text\n",
        "def sorted_suffix(text, sfxArray):\n",
        "  return [text[i:] for i in sfxArray]\n",
        "\n",
        "def map_reads(text, patterns, output_file, num_errors=0, first=True) :\n",
        "    # Add to the end of Genome\n",
        "    text += '$'\n",
        "    \n",
        "    # create output file\n",
        "    if first:\n",
        "      df = pd.DataFrame(list())\n",
        "      df.to_csv(output_file)\n",
        "      with open(output_file, 'w', newline='') as file:\n",
        "        col_names = ['Id', 'Pattern', 'Index(0 Based)']\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(col_names)\n",
        "        file.close()\n",
        "\n",
        "    # Create suffix Array's\n",
        "    sfxArray = suffix_array(text)\n",
        "    print('finished suffix array')\n",
        "    sfxSorted = sorted_suffix(text, sfxArray)\n",
        "    print('finished suffix sorted')\n",
        "\n",
        "    for pattern in patterns:\n",
        "      position = binary_Search(sfxSorted, pattern[1], num_errors)\n",
        "      \n",
        "      if(position != -1):\n",
        "        with open(output_file, 'a', newline='') as file:\n",
        "          writer = csv.writer(file)\n",
        "          if first:\n",
        "            temp = [pattern[0], pattern[2], sfxArray[position]]\n",
        "          else:\n",
        "            temp = [pattern[0], '-', sfxArray[position]] \n",
        "            \n",
        "          writer.writerow(temp)\n",
        "          file.close()\n",
        "    return 1\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0anpD8l26vN"
      },
      "source": [
        "if __name__== '__main__':\n",
        "  genome = read_fasta(\"sequence.fasta\") # TEXT\n",
        "  r_genome = read_fasta(\"sequence.fasta\")\n",
        "  fastq = read_FASTQ(\"ecoli.fastq\") # PATTERNS\n",
        "\n",
        "  map_reads(genome[:3000], fastq, 'output.csv', 0, True)\n",
        "  \n",
        "  # Reverse Complement\n",
        "  r_genome = complement(r_genome[:len(r_genome)-1])\n",
        "  map_reads(r_genome[:3000], fastq, 'output.csv', 0, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAiUeHse26vO"
      },
      "source": [
        "### Data analysis\n",
        "\n",
        "Run your code on the *E. coli* dataset, varying the number of allowed errors between 0 and 3.  Report how the number of reads that you are able to map increases when increasing the number of allowed errors.  How does that match what you know about error rates in Illumina sequencing?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection\n",
        ">For today's analysis I will be using two diffrent files. The first file I want to talk about it the fasta file. The file contains the reference genome of E. coli. The 2nd file is a FastQ file that is compossed of four lines, the first being the sequence identifier. The second line is the raw sequence letters which is followed by a '+' or '-' character. Lastly it has an encoded quality value for the raw sequence in line 2. The FastQ file contains 2500 patterns that I will need to see if they appear in the reference genome. The size of the reference genome is roughly about four million characters. With these two files I will create a Read Mapping Program on Python.\n",
        "\n",
        "# Data Quality Check and Cleaning\n",
        "> I've worked with Fasta files before so I knew exactly what to expect from it. It contains a file description followed by the actual genome. Storing that into a variable was easy. The FastQ file was something new to me. I created a read_FASTQ() function that takes in the file and stores all four lines into a list which was later append to another list. returning a list of lists contating the information wanted by me and needed by Dr. Asa Ben-Hur. The only information I needed was the ID and the pattern. Now that I have the contents needed from both files stored into 2 seprates variables, I am now ready to create my Read Map program.\n",
        "\n",
        "# Data Processing\n",
        "> Now that we got the easy part out of the way it's time to get the real work started. The first thing I do is call my map_reads function where all the data gets processed. Let me explain the image below. map_reads takes in five parameters. the first two being the data I collected from the two files. The third parameter is the name for the output file where I will write what my program ahs collected. The fourth parameter is the number of erros allowed when matching and lastly I pass it a boolean. If it's true then I want map_reads to create an output file, if False then I just want my code to add on the the CSV output file.\n",
        "\n",
        "\n",
        "```\n",
        "# MAIN\n",
        "if __name__== '__main__':\n",
        "  genome = read_fasta(\"sequence.fasta\") # TEXT\n",
        "  fastq = read_FASTQ(\"ecoli.fastq\") # PATTERNS\n",
        "\n",
        "  map_reads(genome, fastq, 'output.csv', 0, True)\n",
        "  \n",
        "  # Reverse Complement \n",
        "  r_genome = complement(genome)\n",
        "  \n",
        "  map_reads(r_genome, fastq, 'output.csv', 0, False)\n",
        "```\n",
        "\n",
        "\n",
        ">Now I want to talk about my function in two different parts. The very first thing I do is add a '$' character at the end of the genome text. This will help me identify the ending of the genome in the suffix array that will be created soon. Before that, I need to create the output file where my data is going to be stored unless if the boolean coming in is false, then I dont create a new file. After that I create two arrays. The first one being the suffix index. The second being the sorted suffix text. The suffix_array function was given to me by Dr. Asa Ben-Hur but the sorted suffix text function was created by me.\n",
        "\n",
        "\n",
        "```\n",
        "# Map Reads first half\n",
        "def map_reads(text, patterns, output_file, num_errors=0, first=True) :\n",
        "    # Add to the end of Genome\n",
        "    text += '$'\n",
        "    \n",
        "    # create output file\n",
        "    if first:\n",
        "      df = pd.DataFrame(list())\n",
        "      df.to_csv(output_file)\n",
        "      with open(output_file, 'w', newline='') as file:\n",
        "        col_names = ['Id', 'Pattern', 'Index(0 Based)']\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(col_names)\n",
        "        file.close()\n",
        "\n",
        "    # Create suffix Array's\n",
        "    sfxArray = suffix_array(text)\n",
        "    print('finished suffix array')\n",
        "    sfxSorted = sorted_suffix(text, sfxArray)\n",
        "    print('finished suffix sorted')\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "# Creates Array of suffix Text\n",
        "def sorted_suffix(text, sfxArray):\n",
        "  return [text[i:] for i in sfxArray]\n",
        "```\n",
        "> The second part is where I go through each pattern in the patterns list and see if they belong in the genome text. I grab the first pattern and send it to a fucntion called binary_Search(). Binary search returns two things only. It will return -1 if the pattern was not found in the genome. It will return an int from 0 to len(genome). This intenger will help tell me where to find pattern in the genome index. If my program does find a pattern then it will go on and open the output file created and write into it 3 things. The Id of the pattern, the symbol '+' or'-', and the index of where the pattern appears in the genome.\n",
        "\n",
        "\n",
        "```\n",
        "# 2nd half\n",
        "    for pattern in patterns:\n",
        "      position = binary_Search(sfxSorted, pattern[1], num_errors)\n",
        "      \n",
        "      if(position != -1):\n",
        "        with open(output_file, 'a', newline='') as file:\n",
        "          writer = csv.writer(file)\n",
        "          if first:\n",
        "            temp = [pattern[0], '+', sfxArray[position]]\n",
        "          else:\n",
        "            temp = [pattern[0], '-', sfxArray[position]] \n",
        "            \n",
        "          writer.writerow(temp)\n",
        "          file.close()\n",
        "```\n",
        ">There are a lot of moving pieces in my code. So I do want to explain each one carefully. I want to start of by explaining my thought process for my binary search function. It takes in three parameters. The first one I pass it the suffix text sorted. The second parameter being the pattern that i'm trying to find. Lastly, its the number of errors the pattern can have. My binary search function is pretty much the same as any other one. We start at the middle and see if the element im looking for is at the top/left of my list or at the bottom/right of the list or of course are we right on the element I am looking for. The only diffrence is where I check for number of errors. It is located in the first part of the if statement. I first check if the number of errors is greater than 0. If it's not then im doing a basic binary search. But if it is greater then I also check if the number of mismatches between the pattern and text are equal to the number of errors. If it passes both of those statements then I move on to splitting up the text and pattern into num_errors + 1 pieces. \n",
        "\n",
        "```\n",
        "# Searches through suffix Text to see if pattern is part of Text\n",
        "def binary_Search(sfxSorted, pattern, num_errors=0):\n",
        "  start = 0\n",
        "  end = len(sfxSorted) - 1\n",
        "  length = len(pattern)\n",
        "\n",
        "\n",
        "  while start <= end:\n",
        "    middle = (start + end) // 2\n",
        "    test = sfxSorted[middle]\n",
        "    midpoint = test[:length]\n",
        "\n",
        "    # Check number of errors\n",
        "    if (sum(c1!=c2 for c1,c2 in zip(pattern,midpoint)) == num_errors and num_errors > 0):\n",
        "      split_pattern = divide_string(pattern, num_errors+1)\n",
        "      split_text = divide_string(midpoint, num_errors+1)\n",
        "      \n",
        "      for i in range(len(split_text)):\n",
        "        if split_pattern[i] == split_text[i]:\n",
        "          return binary_Search(sfxSorted, split_pattern[i], 0)\n",
        "\n",
        "    # Else if midpoint is greater, ignore top/left half\n",
        "    elif(midpoint  > pattern):\n",
        "      end = middle - 1\n",
        "    \n",
        "    # Else if x is smaller, ignore bottom/right half\n",
        "    elif( midpoint < pattern):\n",
        "      start = middle + 1\n",
        "    \n",
        "    # Checks if pattern is present at mid\n",
        "    else:\n",
        "      return top_search(sfxSorted, pattern, middle)\n",
        "\n",
        "  return -1\n",
        "```\n",
        ">To divde the two strings up I created a function called divide_string(). I just take the string and divide into the correct number of pieces depending on the count of num_errors. Then it returns a list with the strings divided up.\n",
        "\n",
        "```\n",
        "# If mismatch found then it breaks string up into n pieces\n",
        "def divide_string(string, n):\n",
        "  chunk = int(len(string) / n) + (len(string) % n > 0)\n",
        "  start = 0\n",
        "  store = []\n",
        "  for i in range(n):\n",
        "    if i == n-1:\n",
        "      store.append(string[start:])\n",
        "    else:\n",
        "      store.append(string[start:start+chunk])\n",
        "\n",
        "    start += chunk\n",
        "\n",
        "  return store\n",
        "```\n",
        ">Now I go through the 2 list that have the strings divided up and I check if list1[0] == list2[0]. If its true then those 2 match up and then I send that shorter pattern back into the binary search except this time I set num_errors to 0 because I know it has no erros so I just need to find it in the genome. When the pattern is found in the suffix text sorted, then I send the sorted list, pattern, and the location of it to a function called top_search(). All top_search() does it find the first occurence of the pattern inside the sorted suffix text. What I do first in it, is to shorten the the strings in the suffix sorted list to the size of the pattern because then it contains just the information that I need. I move up the index if the pattern is matching up with the suffix sorted text. One's it stops matching up then I know where the first occurence is and thats what I return back to binary search which then returns it back to map_reads() function.\n",
        "\n",
        "```\n",
        "# Top_search()\n",
        "def top_search(sfxSorted, pattern, middle):\n",
        "  shorten = [x[:len(pattern)] for x in sfxSorted]\n",
        "  shorten = shorten[:middle+1]\n",
        "\n",
        "  end = len(shorten)-1\n",
        "  firstOccur = end\n",
        "\n",
        "  while pattern == shorten[end]:\n",
        "    firstOccur = end\n",
        "    end -= 1\n",
        "  \n",
        "  return firstOccur\n",
        "```\n",
        "\n",
        "# Data Analysis\n",
        "> Just to exlpain what data my program was able to collect for me, I am gonna run my code with only part of the reference genome text. Below are the parameters I used to collect just a sample.\n",
        "\n",
        "```\n",
        "  map_reads(genome[:3000], fastq, 'output.csv', 0, True)\n",
        "  \n",
        "  # Reverse Complement\n",
        "  r_genome = complement(r_genome[:len(r_genome)-1])\n",
        "  map_reads(r_genome[:3000], fastq, 'output.csv', 0, False)\n",
        "```\n",
        ">Whatever directory my program is in, that is where the output csv file with be created. Below we can see what was collect just with part of the genome.\n",
        "\n",
        "```\n",
        "Id,Pattern,Index(0 Based)\n",
        "@EAS20_8_6_87_1257_1390/1,+,2699\n",
        "@EAS20_8_6_87_1357_2002/1,+,2109\n",
        "@EAS20_8_6_87_1255_1310/1,-,995\n",
        "@EAS20_8_6_87_1261_275/1,-,1400\n",
        "@EAS20_8_6_87_1316_23/1,-,669\n",
        "```\n",
        ">We can see that we got a collection of data. The headers describe what the data is. The first piece is the pattern ID, then we got the pattern Symbol. If it is '+' then that means it found a pattern with the genome untouched. If  it's '-' then that means a pattern was found with the reverse complement genome. Lastly we have the index of the pattern located in the genome. So the first row says that pattern Id: @EAS20_8_6_87_1257_1390/1 was found the genome in index 2699. So let me print out part of the genome at that index.\n",
        "\n",
        "\n",
        "```\n",
        "# Code plus output\n",
        "print(genome[2699:2799])\n",
        "\n",
        "output:\n",
        "GCCGTTGGTACTGCGCGGATATGGTGCGGGCAATGACGTTACAGCTGCCGGTGTCTTTGCTGATCTGCTACGTACCCTCTCATGGAAGTTAGGAGTCTGA\n",
        "```\n",
        ">Now lets go into the fastQ file and see if I can find that pattern ID and if it matches. I went into the fastQ file and did a quick ctr + f. I copied and pasted the pattern Id and this is what I found.\n",
        "\n",
        "```\n",
        "# FastQ file\n",
        "@EAS20_8_6_87_1257_1390/1\n",
        "GCCGTTGGTACTGCGCGGATATGGTGCGGGCAATGACGTTACAGCTGCCGGTGTCTTTGCTGATCTGCTACGTACCCTCTCATGGAAGTTAGGAGTCTGA\n",
        "+\n",
        "HHGFHHFHHHHGHFG8FD=DHHHHHHEHGHHFGHH?EGFFABDFBEEC?H:1A=><DDB?D@4D:HHGHDGHHGG-FG?G;1<8D+2?############\n",
        "```\n",
        ">Now if we compare what pattern my program got and what the FastQ file has for that Id. We get this...\n",
        "\n",
        "\n",
        "```\n",
        "GCCGTTGGTACTGCGCGGATATGGTGCGGGCAATGACGTTACAGCTGCCGGTGTCTTTGCTGATCTGCTACGTACCCTCTCATGGAAGTTAGGAGTCTGA\n",
        "GCCGTTGGTACTGCGCGGATATGGTGCGGGCAATGACGTTACAGCTGCCGGTGTCTTTGCTGATCTGCTACGTACCCTCTCATGGAAGTTAGGAGTCTGA\n",
        "```\n",
        ">Now if we do the same thing with this row \"@EAS20_8_6_87_1255_1310/1,-,995\" we get.\n",
        "\n",
        "\n",
        "```\n",
        "# Code plus output\n",
        "# Code plus output\n",
        "print(r_genome[995:1095])\n",
        "\n",
        "output:\n",
        "ACGAAAGCTAGCATTTAGATACGATGATTTCATCAAACTGTTAACGTGCTACAATTGAACTTGATATATGTCAACGAAGCGTAGTTTTATTGGGTGTCCG\n",
        "```\n",
        ">Lets take a quick look at the FastQ file for this pattern.\n",
        "\n",
        "\n",
        "```\n",
        "# FastQ file\n",
        "@EAS20_8_6_87_1255_1310/1\n",
        "ACGAAAGCTAGCATTTAGATACGATGATTTCATCAAACTGTTAACGTGCTACAATTGAACTTGATATATGTCAACGAAGCGTAGTTTTATTGGGTGTCCG\n",
        "+\n",
        "FBFFEHHHHHHEHHHHHHHHHGGHHHHHHHFHHHGHHHHHGHHHFHHHGHHHGHGHHFHHHHFHHHHDHHHHHFFHGGHDHEHHHHGHHHCBHDEHHGEH\n",
        "```\n",
        ">Compare\n",
        "\n",
        "```\n",
        "ACGAAAGCTAGCATTTAGATACGATGATTTCATCAAACTGTTAACGTGCTACAATTGAACTTGATATATGTCAACGAAGCGTAGTTTTATTGGGTGTCCG\n",
        "ACGAAAGCTAGCATTTAGATACGATGATTTCATCAAACTGTTAACGTGCTACAATTGAACTTGATATATGTCAACGAAGCGTAGTTTTATTGGGTGTCCG\n",
        "```\n",
        ">My program was able to find the location of the pattern in the reference genome using suffix array!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mvTslxst2PcP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf8oG9zk26vO"
      },
      "source": [
        "### Submission\n",
        "\n",
        "Submit your assignment as a Jupyter notebook via Canvas.  \n",
        "\n",
        "### Grading \n",
        "\n",
        "Here is what the grade sheet will look like for this assignment. \n",
        "\n",
        "```\n",
        "Grading sheet for assignment 1\n",
        "- Correctness of map_reads (80 pts)\n",
        "- Results analysis and discussion (20 pts)\n",
        "```\n"
      ]
    }
  ]
}